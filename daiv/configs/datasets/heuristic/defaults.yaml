 # Copyright (c) 2022, salesforce.com, inc.
 # All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause

datasets:
  heuristic_vqa:
    # data_dir: ${env.data_dir}/datasets
    data_type: images # [images|videos|features]

    build_info:
      # Be careful not to append minus sign (-) before split to avoid itemizing
      annotations:
        train:
          url:
              # TODO make this order insensitive
              # - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/okvqa_train.json
              # - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/OpenEnded_mscoco_train2014_questions.json
              # - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/mscoco_train2014_annotations.json
          storage:
              - /root/workspace/24s-VQA-MLLM/BEiT3/stage2-t5/VQA-MLLM-stage2/daiv/data/okvqa/okvqa_train.json #question
              - /root/workspace/24s-VQA-MLLM/dataset/a-okvqa/aokvqa_v1p0_train_encoded.json #heuristic prompt
              - /root/datasets/okvqa/data/assets/captions_okvqa.json #question caption
              - /root/datasets/okvqa/data/assets/captions_aokvqa.json #heuristic prompt caption
              - /root/workspace/24s-VQA-MLLM/BEiT3/24s-VQA-MLLM/outputs/results/beit3-base/candidates.json
              - /root/workspace/24s-VQA-MLLM/BEiT3/24s-VQA-MLLM/outputs/results/beit3-base/examples.json
        test:
          url:
              # TODO make this order insensitive
              # - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/okvqa_val_eval.json
              # - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/okvqa_answer_list_train.json
              # - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/OpenEnded_mscoco_val2014_questions.json
              # - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/mscoco_val2014_annotations.json
          storage:
              - /root/workspace/24s-VQA-MLLM/BEiT3/stage2-t5/VQA-MLLM-stage2/daiv/data/okvqa/okvqa_val.json
              - /root/workspace/24s-VQA-MLLM/BEiT3/stage2-t5/VQA-MLLM-stage2/daiv/data/okvqa/okvqa_train.json #heuristic prompt
              - /root/datasets/okvqa/data/assets/captions_okvqa.json #question caption
              # - /root/workspace/24s-VQA-MLLM/BEiT3/24s-VQA-MLLM/outputs/results/beit3-large-okvqa/candidates.json
              # - /root/workspace/24s-VQA-MLLM/BEiT3/24s-VQA-MLLM/outputs/results/beit3-large-okvqa/examples.json
              - /root/workspace/24s-VQA-MLLM/BEiT3/assets/candidates_okvqa.json
              - /root/workspace/24s-VQA-MLLM/BEiT3/assets/answer_aware_examples_okvqa.json
              # - /root/datasets/okvqa/data/assets/answer_dict_okvqa.json
      images:
          storage: /root/datasets/okvqa/data
      
      # heuristics:
      #     candidates: /root/workspace/24s-VQA-MLLM/BEiT3/24s-VQA-MLLM/outputs/results/beit3-base/candidates.json
      #     examples: /root/workspace/24s-VQA-MLLM/BEiT3/24s-VQA-MLLM/outputs/results/beit3-base/examples.json