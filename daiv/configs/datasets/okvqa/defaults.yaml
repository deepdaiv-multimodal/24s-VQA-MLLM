 # Copyright (c) 2022, salesforce.com, inc.
 # All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause

datasets:
  ok_vqa:
    #data_dir: ${env.data_dir}/datasets
    # data_type: images # [images|videos|features]
    # build_info:
    #   storage: daiv/data/raw_data #daiv/data/okvqa
    #   vis_root: /root/datasets/okvqa/data/train2014
    # build_info:
    #   Be careful not to append minus sign (-) before split to avoid itemizing
    #   annotations:
    #     train:
    #       url:
    #           TODO make this order insensitive
    #           - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/okvqa_train.json
    #           - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/OpenEnded_mscoco_train2014_questions.json
    #           - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/mscoco_train2014_annotations.json
    #       storage:
    #           - data/okvqa/annotations/okvqa_train.json
    #           - okvqa/annotations/OpenEnded_mscoco_train2014_questions.json
    #           - okvqa/annotations/mscoco_train2014_annotations.json
    #     test:
    #       url:
    #           TODO make this order insensitive
    #           - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/okvqa_val_eval.json
    #           - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/okvqa_answer_list_train.json
    #           - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/OpenEnded_mscoco_val2014_questions.json
    #           - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/mscoco_val2014_annotations.json
    #       storage:
    #           - data/okvqa/annotations/vqa_val_eval.json
    #           - data/okvqa/annotations/answer_list.json
    #           - data/okvqa/annotations/OpenEnded_mscoco_val2014_questions.json
    #           - data/okvqa/annotations/mscoco_val2014_annotations.json
    #   images:
    #       storage: data/coco/images/

    # data_dir: ${env.data_dir}/datasets
    data_type: images # [images|videos|features]

    build_info:
      # Be careful not to append minus sign (-) before split to avoid itemizing
      annotations:
        train:
          url:
              # TODO make this order insensitive
              - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/okvqa_train.json
              # - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/OpenEnded_mscoco_train2014_questions.json
              # - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/mscoco_train2014_annotations.json
          storage:
              - /root/workspace/24s-VQA-MLLM/BEiT3/stage2-t5/VQA-MLLM-stage2/daiv/data/okvqa/okvqa_train.json
              # - okvqa/annotations/OpenEnded_mscoco_train2014_questions.json
              # - okvqa/annotations/mscoco_train2014_annotations.json
        test:
          url:
              # TODO make this order insensitive
              - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/okvqa_val_eval.json
              - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/okvqa_answer_list_train.json
              - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/OpenEnded_mscoco_val2014_questions.json
              - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/okvqa/mscoco_val2014_annotations.json
          storage:
              # - okvqa/annotations/vqa_val_eval.json
              # - okvqa/annotations/answer_list.json
              # - okvqa/annotations/OpenEnded_mscoco_val2014_questions.json
              # - okvqa/annotations/mscoco_val2014_annotations.json
              - /root/workspace/24s-VQA-MLLM/BEiT3/stage2-t5/VQA-MLLM-stage2/daiv/data/okvqa/okvqa_val.json #testing 
              - /root/datasets/okvqa/data/assets/answer_dict_okvqa.json
              - /root/datasets/okvqa/data/okvqa/OpenEnded_mscoco_val2014_questions.json
              - /root/datasets/okvqa/data/okvqa/mscoco_val2014_annotations.json
      images:
          storage: /root/datasets/okvqa/data