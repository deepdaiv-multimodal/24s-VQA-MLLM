{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"EgaRqvazIvY4"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1dzay5UPJFeE"},"outputs":[],"source":["%cd /content/drive/MyDrive/prophet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mX3afxHLLdwq"},"outputs":[],"source":["#필요패키지 install\n","!pip install pyyaml==6.0\n","!pip install einops==0.6.0\n","!pip install huggingface-hub==0.12.1\n","!pip install openai==0.18.0\n","!pip install opencv-python==4.5.5.64\n","!pip install pillow==9.3.0\n","!pip install pyyaml==6.0\n","!pip install sentence-transformers==2.2.2\n","!pip install sentencepiece==0.1.96\n","!pip install tokenizers==0.11.6\n","!pip install tqdm==4.63.0\n","!pip install transformers==4.26.1\n","!pip install git+https://github.com/openai/CLIP.git\n","!pip install git+https://github.com/huggingface/transformers\n","!pip install torchscale"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Pu1ad8CUogG"},"outputs":[],"source":["!pip install tensorboardX\n","!pip install torchmetrics\n","!pip install deepspeed==0.4.0\n","!pip install mpi4py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"mg6jjVJ8EQFw","outputId":"9ac3e37c-6716-47e9-cfb0-731d6a6be903"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading common data...\n","== Total image number: 14031\n","== Answer vocab size: 4477\n","Common data process is done.\n","\n","2024-07-01 10:59:49.521147: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-01 10:59:49.521203: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-01 10:59:49.522792: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-01 10:59:49.530831: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-07-01 10:59:50.706682: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/usr/local/lib/python3.10/dist-packages/transformers/models/beit/feature_extraction_beit.py:28: FutureWarning: The class BeitFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use BeitImageProcessor instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/utils/deprecation.py:165: UserWarning: The following named arguments are not valid for `BeitFeatureExtractor.__init__` and were ignored: 'feature_extractor_type'\n","  return func(*args, **kwargs)\n","Loaded pretrained weights from /content/drive/MyDrive/prophet/datasets/beit3/beit3_base_patch16_224.pth\n","[2024-07-01 10:59:57,308] [INFO] [logging.py:60:log_dist] [Rank -1] DeepSpeed info: version=0.4.0, git-hash=unknown, git-branch=unknown\n","[2024-07-01 10:59:57,309] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...\n","[2024-07-01 10:59:57,697] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.28.0.12, master_port=29500\n","[2024-07-01 10:59:57,698] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl\n","[2024-07-01 10:59:58,076] [INFO] [utils.py:11:_initialize_parameter_parallel_groups] data_parallel_size: 1, parameter_parallel_size: 1\n","[2024-07-01 10:59:58,399] [INFO] [engine.py:172:__init__] DeepSpeed Flops Profiler Enabled: False\n","[2024-07-01 10:59:58,399] [INFO] [engine.py:682:_configure_optimizer] Removing param_group that has no 'params' in the client Optimizer\n","[2024-07-01 10:59:58,399] [INFO] [engine.py:687:_configure_optimizer] Using client Optimizer as basic optimizer\n","[2024-07-01 10:59:58,399] [INFO] [engine.py:696:_configure_optimizer] DeepSpeed Basic Optimizer = AdamW\n","[2024-07-01 10:59:58,400] [INFO] [logging.py:60:log_dist] [Rank 0] Creating fp16 unfused optimizer with dynamic loss scale\n","[2024-07-01 10:59:58,400] [INFO] [unfused_optimizer.py:37:__init__] Fused Lamb Legacy : False \n","[2024-07-01 10:59:58,573] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n","[2024-07-01 10:59:58,573] [INFO] [engine.py:509:_configure_lr_scheduler] DeepSpeed using client LR scheduler\n","[2024-07-01 10:59:58,573] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n","[2024-07-01 10:59:58,573] [INFO] [logging.py:60:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]\n","[2024-07-01 10:59:58,573] [INFO] [config.py:900:print] DeepSpeedEngine configuration:\n","[2024-07-01 10:59:58,574] [INFO] [config.py:904:print]   activation_checkpointing_config  {\n","    \"partition_activations\": false, \n","    \"contiguous_memory_optimization\": false, \n","    \"cpu_checkpointing\": false, \n","    \"number_checkpoints\": null, \n","    \"synchronize_checkpoint_boundary\": false, \n","    \"profile\": false\n","}\n","[2024-07-01 10:59:58,574] [INFO] [config.py:904:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n","[2024-07-01 10:59:58,574] [INFO] [config.py:904:print]   allreduce_always_fp32 ........ False\n","[2024-07-01 10:59:58,574] [INFO] [config.py:904:print]   amp_enabled .................. False\n","[2024-07-01 10:59:58,574] [INFO] [config.py:904:print]   amp_params ................... False\n","[2024-07-01 10:59:58,574] [INFO] [config.py:904:print]   checkpoint_tag_validation_enabled  True\n","[2024-07-01 10:59:58,574] [INFO] [config.py:904:print]   checkpoint_tag_validation_fail  False\n","[2024-07-01 10:59:58,574] [INFO] [config.py:904:print]   disable_allgather ............ False\n","[2024-07-01 10:59:58,574] [INFO] [config.py:904:print]   dump_state ................... False\n","[2024-07-01 10:59:58,574] [INFO] [config.py:904:print]   dynamic_loss_scale_args ...... {'init_scale': 4294967296, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}\n","[2024-07-01 10:59:58,574] [INFO] [config.py:904:print]   eigenvalue_enabled ........... False\n","[2024-07-01 10:59:58,574] [INFO] [config.py:904:print]   eigenvalue_gas_boundary_resolution  1\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   eigenvalue_layer_name ........ bert.encoder.layer\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   eigenvalue_layer_num ......... 0\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   eigenvalue_max_iter .......... 100\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   eigenvalue_stability ......... 1e-06\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   eigenvalue_tol ............... 0.01\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   eigenvalue_verbose ........... False\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   elasticity_enabled ........... False\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   flops_profiler_config ........ {\n","    \"enabled\": false, \n","    \"profile_step\": 1, \n","    \"module_depth\": -1, \n","    \"top_modules\": 1, \n","    \"detailed\": true, \n","    \"output_file\": null\n","}\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   fp16_enabled ................. True\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   fp16_mixed_quantize .......... False\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   global_rank .................. 0\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   gradient_accumulation_steps .. 1\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   gradient_clipping ............ 0.0\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   gradient_predivide_factor .... 1.0\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   initial_dynamic_scale ........ 4294967296\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   loss_scale ................... 0\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   memory_breakdown ............. False\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   optimizer_legacy_fusion ...... False\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   optimizer_name ............... adamw\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   optimizer_params ............. {'lr': 5e-05, 'betas': [0.9, 0.999], 'weight_decay': 0.01}\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   pld_enabled .................. False\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   pld_params ................... False\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   prescale_gradients ........... False\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   quantize_change_rate ......... 0.001\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   quantize_groups .............. 1\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   quantize_offset .............. 1000\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   quantize_period .............. 1000\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   quantize_rounding ............ 0\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   quantize_start_bits .......... 16\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   quantize_target_bits ......... 8\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   quantize_training_enabled .... False\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   quantize_type ................ 0\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   quantize_verbose ............. False\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   scheduler_name ............... None\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   scheduler_params ............. None\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   sparse_attention ............. None\n","[2024-07-01 10:59:58,575] [INFO] [config.py:904:print]   sparse_gradients_enabled ..... False\n","[2024-07-01 10:59:58,576] [INFO] [config.py:904:print]   steps_per_print .............. 1000\n","[2024-07-01 10:59:58,576] [INFO] [config.py:904:print]   tensorboard_enabled .......... False\n","[2024-07-01 10:59:58,576] [INFO] [config.py:904:print]   tensorboard_job_name ......... DeepSpeedJobName\n","[2024-07-01 10:59:58,576] [INFO] [config.py:904:print]   tensorboard_output_path ...... \n","[2024-07-01 10:59:58,576] [INFO] [config.py:904:print]   train_batch_size ............. 8\n","[2024-07-01 10:59:58,576] [INFO] [config.py:904:print]   train_micro_batch_size_per_gpu  8\n","[2024-07-01 10:59:58,576] [INFO] [config.py:904:print]   use_quantizer_kernel ......... False\n","[2024-07-01 10:59:58,576] [INFO] [config.py:904:print]   wall_clock_breakdown ......... False\n","[2024-07-01 10:59:58,576] [INFO] [config.py:904:print]   world_size ................... 1\n","[2024-07-01 10:59:58,576] [INFO] [config.py:904:print]   zero_allow_untested_optimizer  False\n","[2024-07-01 10:59:58,576] [INFO] [config.py:904:print]   zero_config .................. {\n","    \"stage\": 0, \n","    \"contiguous_gradients\": false, \n","    \"reduce_scatter\": true, \n","    \"reduce_bucket_size\": 5.000000e+08, \n","    \"allgather_partitions\": true, \n","    \"allgather_bucket_size\": 5.000000e+08, \n","    \"overlap_comm\": false, \n","    \"load_from_fp32_weights\": true, \n","    \"elastic_checkpoint\": true, \n","    \"offload_param\": null, \n","    \"offload_optimizer\": null, \n","    \"sub_group_size\": 1.000000e+12, \n","    \"prefetch_bucket_size\": 5.000000e+07, \n","    \"param_persistence_threshold\": 1.000000e+05, \n","    \"max_live_parameters\": 1.000000e+09, \n","    \"max_reuse_distance\": 1.000000e+09, \n","    \"gather_fp16_weights_on_model_save\": false, \n","    \"ignore_unused_parameters\": true, \n","    \"legacy_stage1\": false\n","}\n","[2024-07-01 10:59:58,576] [INFO] [config.py:904:print]   zero_enabled ................. False\n","[2024-07-01 10:59:58,576] [INFO] [config.py:904:print]   zero_optimization_stage ...... 0\n","[2024-07-01 10:59:58,576] [INFO] [config.py:906:print]   json = {\n","    \"train_batch_size\": 8, \n","    \"gradient_accumulation_steps\": 1, \n","    \"steps_per_print\": 1000, \n","    \"optimizer\": {\n","        \"type\": \"AdamW\", \n","        \"params\": {\n","            \"lr\": 5e-05, \n","            \"betas\": [0.9, 0.999], \n","            \"weight_decay\": 0.01\n","        }\n","    }, \n","    \"fp16\": {\n","        \"enabled\": true, \n","        \"loss_scale\": 0, \n","        \"initial_scale_power\": 32, \n","        \"loss_scale_window\": 1000, \n","        \"hysteresis\": 2, \n","        \"min_loss_scale\": 1\n","    }, \n","    \"logging_level\": \"ERROR\"\n","}\n","Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n","Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/utils/build.ninja...\n","Building extension module utils...\n","Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n","ninja: no work to do.\n","Loading extension module utils...\n","Time to load utils op: 0.08482551574707031 seconds\n","/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n","  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/utils.py:133: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n","  overflow_gpu = torch.cuda.ByteTensor([overflow])\n","[2024-07-01 11:00:01,061] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 0\n","[2024-07-01 11:00:01,062] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 4294967296 to 2147483648.0\n","[2024-07-01 11:00:01,062] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0\n","Epoch 0, Step 0, Loss 0.699302077293396\n","[2024-07-01 11:00:01,190] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 1\n","[2024-07-01 11:00:01,190] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 2147483648.0 to 1073741824.0\n","[2024-07-01 11:00:01,190] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0\n","[2024-07-01 11:00:01,275] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 2\n","[2024-07-01 11:00:01,275] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 1073741824.0 to 536870912.0\n","[2024-07-01 11:00:01,275] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0\n","[2024-07-01 11:00:01,361] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 3\n","[2024-07-01 11:00:01,361] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 536870912.0 to 268435456.0\n","[2024-07-01 11:00:01,362] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0\n","[2024-07-01 11:00:01,444] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 4\n","[2024-07-01 11:00:01,444] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 268435456.0 to 134217728.0\n","[2024-07-01 11:00:01,445] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0\n","[2024-07-01 11:00:01,526] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 5\n","[2024-07-01 11:00:01,526] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 134217728.0 to 67108864.0\n","[2024-07-01 11:00:01,526] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0\n","[2024-07-01 11:00:01,608] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 6\n","[2024-07-01 11:00:01,608] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 67108864.0 to 33554432.0\n","[2024-07-01 11:00:01,608] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0\n","[2024-07-01 11:00:01,692] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 7\n","[2024-07-01 11:00:01,693] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 33554432.0 to 16777216.0\n","[2024-07-01 11:00:01,693] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0\n","[2024-07-01 11:00:01,778] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 8\n","[2024-07-01 11:00:01,778] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 16777216.0 to 8388608.0\n","[2024-07-01 11:00:01,778] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0\n","[2024-07-01 11:00:01,863] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 9\n","[2024-07-01 11:00:01,863] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 8388608.0 to 4194304.0\n","[2024-07-01 11:00:01,863] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0\n","[2024-07-01 11:00:01,945] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 10\n","[2024-07-01 11:00:01,945] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 4194304.0 to 2097152.0\n","[2024-07-01 11:00:01,945] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0\n","[2024-07-01 11:00:02,026] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 11\n","[2024-07-01 11:00:02,026] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 2097152.0 to 1048576.0\n","[2024-07-01 11:00:02,026] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0\n","[2024-07-01 11:00:02,108] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 12\n","[2024-07-01 11:00:02,108] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0\n","[2024-07-01 11:00:02,108] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0\n","[2024-07-01 11:00:02,190] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 13\n","[2024-07-01 11:00:02,190] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0\n","[2024-07-01 11:00:02,190] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0\n","[2024-07-01 11:00:02,272] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 14\n","[2024-07-01 11:00:02,272] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0\n","[2024-07-01 11:00:02,272] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0\n","[2024-07-01 11:00:02,355] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 15\n","[2024-07-01 11:00:02,355] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0\n","[2024-07-01 11:00:02,355] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0\n","[2024-07-01 11:00:02,439] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 16\n","[2024-07-01 11:00:02,439] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0\n","[2024-07-01 11:00:02,439] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0\n","[2024-07-01 11:05:41,024] [INFO] [logging.py:60:log_dist] [Rank 0] step=1000, skipped=17, lr=[5e-05], mom=[(0.9, 0.999)]\n","[2024-07-01 11:05:41,107] [INFO] [timer.py:157:stop] 0/1000, SamplesPerSec=31.484413015940106\n","[2024-07-01 11:05:45,639] [INFO] [unfused_optimizer.py:267:_update_scale] No Grad overflow for 1000 iterations\n","[2024-07-01 11:05:45,639] [INFO] [unfused_optimizer.py:269:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0\n","[2024-07-01 11:05:45,872] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 1018\n","[2024-07-01 11:05:45,872] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0\n","[2024-07-01 11:05:45,872] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0\n","[2024-07-01 11:10:00,494] [INFO] [logging.py:60:log_dist] [Rank 0] step=2000, skipped=18, lr=[5e-05], mom=[(0.9, 0.999)]\n","[2024-07-01 11:10:00,577] [INFO] [timer.py:157:stop] 0/2000, SamplesPerSec=31.336462791159363\n","Epoch 0, Step 2000, Loss 0.0038559935055673122\n","[2024-07-01 11:10:05,631] [INFO] [unfused_optimizer.py:267:_update_scale] No Grad overflow for 1000 iterations\n","[2024-07-01 11:10:05,631] [INFO] [unfused_optimizer.py:269:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0\n","[2024-07-01 11:10:05,863] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 2020\n","[2024-07-01 11:10:05,863] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0\n","[2024-07-01 11:10:05,863] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0\n","[2024-07-01 11:14:19,377] [INFO] [logging.py:60:log_dist] [Rank 0] step=3000, skipped=19, lr=[5e-05], mom=[(0.9, 0.999)]\n","[2024-07-01 11:14:19,459] [INFO] [timer.py:157:stop] 0/3000, SamplesPerSec=31.30749995466776\n","[2024-07-01 11:14:25,016] [INFO] [unfused_optimizer.py:267:_update_scale] No Grad overflow for 1000 iterations\n","[2024-07-01 11:14:25,017] [INFO] [unfused_optimizer.py:269:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0\n","[2024-07-01 11:14:25,251] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 3022\n","[2024-07-01 11:14:25,252] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0\n","[2024-07-01 11:14:25,252] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0\n","[2024-07-01 11:18:38,354] [INFO] [logging.py:60:log_dist] [Rank 0] step=4000, skipped=20, lr=[5e-05], mom=[(0.9, 0.999)]\n","[2024-07-01 11:18:38,437] [INFO] [timer.py:157:stop] 0/4000, SamplesPerSec=31.290391864889266\n","Epoch 0, Step 4000, Loss 0.0020463967230170965\n","[2024-07-01 11:18:44,623] [INFO] [unfused_optimizer.py:267:_update_scale] No Grad overflow for 1000 iterations\n","[2024-07-01 11:18:44,623] [INFO] [unfused_optimizer.py:269:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0\n","[2024-07-01 11:18:44,855] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 4024\n","[2024-07-01 11:18:44,855] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0\n","[2024-07-01 11:18:44,855] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0\n","[2024-07-01 11:22:57,075] [INFO] [logging.py:60:log_dist] [Rank 0] step=5000, skipped=21, lr=[5e-05], mom=[(0.9, 0.999)]\n","[2024-07-01 11:22:57,158] [INFO] [timer.py:157:stop] 0/5000, SamplesPerSec=31.28520187712475\n","[2024-07-01 11:23:03,715] [INFO] [unfused_optimizer.py:267:_update_scale] No Grad overflow for 1000 iterations\n","[2024-07-01 11:23:03,716] [INFO] [unfused_optimizer.py:269:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0\n","[2024-07-01 11:23:03,949] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 5026\n","[2024-07-01 11:23:03,949] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0\n","[2024-07-01 11:23:03,949] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0\n","[2024-07-01 11:27:15,941] [INFO] [logging.py:60:log_dist] [Rank 0] step=6000, skipped=22, lr=[5e-05], mom=[(0.9, 0.999)]\n","[2024-07-01 11:27:16,024] [INFO] [timer.py:157:stop] 0/6000, SamplesPerSec=31.27928364228784\n","Epoch 0, Step 6000, Loss 0.0025692081544548273\n","[2024-07-01 11:27:23,079] [INFO] [unfused_optimizer.py:267:_update_scale] No Grad overflow for 1000 iterations\n","[2024-07-01 11:27:23,079] [INFO] [unfused_optimizer.py:269:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0\n","[2024-07-01 11:27:23,313] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 6028\n","[2024-07-01 11:27:23,313] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0\n","[2024-07-01 11:27:23,313] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0\n","[2024-07-01 11:31:35,108] [INFO] [logging.py:60:log_dist] [Rank 0] step=7000, skipped=23, lr=[5e-05], mom=[(0.9, 0.999)]\n","[2024-07-01 11:31:35,191] [INFO] [timer.py:157:stop] 0/7000, SamplesPerSec=31.270212705434716\n","[2024-07-01 11:31:42,764] [INFO] [unfused_optimizer.py:267:_update_scale] No Grad overflow for 1000 iterations\n","[2024-07-01 11:31:42,765] [INFO] [unfused_optimizer.py:269:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0\n","[2024-07-01 11:31:42,999] [INFO] [unfused_optimizer.py:257:_update_scale] Grad overflow on iteration: 7030\n","[2024-07-01 11:31:42,999] [INFO] [unfused_optimizer.py:258:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0\n","[2024-07-01 11:31:42,999] [INFO] [unfused_optimizer.py:181:step] [deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0\n"]}],"source":["!bash scripts/finetune.sh --task ok --version okvqa_finetune_1 --gpu 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_gfXUyXL6Aq7"},"outputs":[],"source":["# DeepSpeed 소스 코드 디렉토리로 이동\n","%cd /usr/local/lib/python3.10/dist-packages/deepspeed\n","\n","# collections.abc를 사용하도록 변경\n","!sed -i 's/collections.Sequence/collections.abc.Sequence/' $(grep -rl 'collections.Sequence' .)\n","!sed -i 's/collections.Mapping/collections.abc.Mapping/' $(grep -rl 'collections.Mapping' .)\n","\n","# 확인\n","!grep -r 'collections.abc.Sequence' .\n","!grep -r 'collections.abc.Mapping' .\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8btxlP8S5a1"},"outputs":[],"source":["# DeepSpeed 소스 코드 디렉토리로 이동\n","%cd /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/\n","\n","# torch._six를 사용하는 파일에서 해당 부분을 math 모듈로 변경\n","!sed -i 's/from torch._six import inf/from torch import inf/' utils.py\n","\n","# 변경 내용 확인 (선택 사항)\n","!grep 'from torch import inf' utils.py\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mHpNYUPtTkof"},"outputs":[],"source":["# DeepSpeed 소스 코드 디렉토리로 이동\n","%cd /usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/\n","\n","# torch._six를 사용하는 파일에서 해당 부분을 math 모듈로 변경\n","!sed -i 's/from torch._six import inf/from torch import inf/' stage2.py\n","\n","# 변경 내용 확인 (선택 사항)\n","!grep 'from torch import inf' stage2.py\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}